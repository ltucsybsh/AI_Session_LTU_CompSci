{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI Chatbot Activity Guide for Students\n",
        "\n",
        "### Activity 1: Basic Chatbot Interaction\n",
        "\n",
        "**Aim:**\n",
        "Learn how to interact with an AI-powered chatbot by entering simple questions or greetings.\n",
        "\n",
        "**Shortfall:**\n",
        "You might notice that the chatbot gives unexpected or unrelated responses when given very general or open-ended questions.\n",
        "\n",
        "**Resolution in Next Activity:**\n",
        "To address this, you'll learn about conversational context in the next step, helping the chatbot generate better responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "C4ft9rQ_QOq1",
        "outputId": "f04bbf52-8271-49b6-c775-8e6a2058669c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained DialoGPT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "print(\"ðŸ¤– Chatbot is ready! Type 'quit' to stop.\")\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history_ids = None\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        print(\"ðŸ‘‹ Chatbot session ended.\")\n",
        "        break\n",
        "\n",
        "    # Encode user input and append to chat history\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Append tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "    # Generate response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_k=100,\n",
        "        temperature=0.75\n",
        "    )\n",
        "\n",
        "    # Decode and print response\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"Chatbot: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activity 2: Improving Attention Masks\n",
        "\n",
        "**Aim:**\n",
        "Resolve the attention mask problem by explicitly defining masks, making the responses more coherent.\n",
        "\n",
        "**Shortfall:**\n",
        "DialoGPT still gives conversational but nonsensical responses when asked factual or simple questions.\n",
        "\n",
        "**Resolution in Next Activity:**\n",
        "In the following activity, you'll switch to GPT-Neo, a model more suited to general knowledge questions and clearer responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "yLyQu786RS0H",
        "outputId": "391023c4-670d-4602-f8dd-beb38c794a0d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load DialoGPT-medium model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Ensure pad_token_id is defined properly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"ðŸ¤– Chatbot ready! Type 'quit' to exit.\")\n",
        "\n",
        "# Start chat\n",
        "chat_history_ids = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        print(\"ðŸ‘‹ Chatbot session ended.\")\n",
        "        break\n",
        "\n",
        "    # Encode user input and manage attention mask\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Append user input to chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
        "\n",
        "    # Create attention mask explicitly\n",
        "    attention_mask = torch.ones(bot_input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    # Generate a response from DialoGPT\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode and display the response\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"Chatbot: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activity 3: Chatbot using GPT-Neo\n",
        "\n",
        "**Aim:**\n",
        "Build an interactive chatbot with GPT-Neo for better responses to general-knowledge and factual questions.\n",
        "\n",
        "**Shortfall:**\n",
        "GPT-Neo responses may be slow and sometimes irrelevant due to its nature as a general text-generation model, not optimized specifically for conversational interaction.\n",
        "\n",
        "**Resolution in Next Activity:**\n",
        "The next activity introduces a professional-grade conversational model using Google's Gemini API, enhancing response quality significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "GNSXuczbSbnS",
        "outputId": "d672e505-3649-498d-d44d-3b67a8dd8293"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-Neo\n",
        "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "print(\"ðŸ¤– AI is ready! Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nYou: \")\n",
        "    if question.lower() in ['quit', 'exit']:\n",
        "        print(\"ðŸ‘‹ Ending session.\")\n",
        "        break\n",
        "\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_length=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        truncation=True  # explicitly truncates long inputs\n",
        "    )\n",
        "\n",
        "    answer = response[0]['generated_text'].split('A:')[1].split('Q:')[0].strip()\n",
        "    print(f\"AI: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activity 4: Gemini API Chatbot (Free Version)\n",
        "\n",
        "**Aim:**\n",
        "Set up a chatbot using Google's Gemini API, significantly improving response coherence, speed, and reliability.\n",
        "\n",
        "**Shortfall:**\n",
        "Initial implementation might not handle conversational context optimally, potentially losing the conversation history or context.\n",
        "\n",
        "**Resolution in Next Activity:**\n",
        "The final activity implements an improved conversation management system provided by Gemini to ensure coherent multi-turn conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QI69B6u4TtY9",
        "outputId": "f2574b2f-2b84-4a80-86a8-a22411737a9c"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Replace with your actual API key\n",
        "genai.configure(api_key=\"Your_API_Key\")\n",
        "\n",
        "\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "\n",
        "def chat_with_gemini():\n",
        "    conversation_history = []\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        conversation_history.append({\"role\": \"user\", \"parts\": user_input})\n",
        "\n",
        "        try:\n",
        "            response = model.generate_content(conversation_history)\n",
        "            gemini_response = response.text\n",
        "            print(\"Gemini:\", gemini_response)\n",
        "            conversation_history.append({\"role\": \"model\", \"parts\": gemini_response})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "chat_with_gemini()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activity 5: Optimized Gemini Conversational Chatbot\n",
        "\n",
        "**Aim:**\n",
        "Enhance the Gemini chatbot using Gemini's built-in conversational management (`start_chat`) for smooth, multi-turn interactions.\n",
        "\n",
        "**Benefits:**\n",
        "- Seamless management of conversational context\n",
        "- Highly coherent and accurate responses\n",
        "- User-friendly and simplified code structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure your Gemini API key\n",
        "genai.configure(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "\n",
        "def chat_with_gemini():\n",
        "    conversation = model.start_chat(history=[])\n",
        "\n",
        "    print(\"ðŸ¤– Gemini AI Chatbot is ready! (type 'exit' to quit)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"ðŸ‘‹ Session ended.\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = conversation.send_message(user_input)\n",
        "            print(f\"Gemini: {response.text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "chat_with_gemini()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "\n",
        "Through these activities, you've explored common chatbot issues and progressively resolved them by adopting advanced models and proper conversational context management. You've also explored selecting appropriate AI models based on your conversational needs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
